{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run the benchmarks and visualize\n",
    "The only parts of the code you must change is the number of agents (2,3,4), and the grid size (6,10).\n",
    "After changing those, you can add the path to the corresponding model in the line containing \n",
    "\"mu_target.load_state_dict\". Other settings can also be changed. \n",
    "The three models are, in order, Distance observations, No observations, and All observations.\n",
    "Render is one by default, but can be turned off by modifying the \"test(\" function to render=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from ma_gym.wrappers import Monitor\n",
    "from datetime import datetime\n",
    "\n",
    "class MuNet(nn.Module):\n",
    "    # Actor Network\n",
    "    def __init__(self, observation_space, action_space, weight_multiplier = 1):\n",
    "        super(MuNet, self).__init__()\n",
    "        self.num_agents = len(observation_space)\n",
    "        self.action_space = action_space\n",
    "        for agent_i in range(self.num_agents): # For each agent, instantiate a Individual Actor network\n",
    "            n_obs = observation_space[agent_i].shape[0] + (self.num_agents-1) # To add the distance observation from all the other agents\n",
    "            print('N_obs in MuNet',n_obs)\n",
    "            num_action = action_space[agent_i].n\n",
    "            setattr(self, 'agent_{}'.format(agent_i), nn.Sequential(nn.Linear(n_obs, int(128*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(128*weight_multiplier), int(64*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(64*weight_multiplier), num_action)))\n",
    "\n",
    "    def forward(self, obs): # Returns the actions of the agents\n",
    "        action_logits = [torch.empty(1, _.n) for _ in self.action_space]\n",
    "        for agent_i in range(self.num_agents):\n",
    "            x = getattr(self, 'agent_{}'.format(agent_i))(obs[:, agent_i, :]).unsqueeze(1)\n",
    "            action_logits[agent_i] = x\n",
    "\n",
    "        return torch.cat(action_logits, dim=1)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    # Critic network\n",
    "    def __init__(self, observation_space, action_space,weight_multiplier = 1):\n",
    "        super(QNet, self).__init__()\n",
    "        self.num_agents = len(observation_space)\n",
    "        total_action = sum([_.n for _ in action_space])\n",
    "        total_obs = sum([_.shape[0] for _ in observation_space]) + self.num_agents*(self.num_agents-1)\n",
    "        print(\"Total_obs in QNet\",total_obs)\n",
    "        for agent_i in range(self.num_agents):\n",
    "            setattr(self, 'agent_{}'.format(agent_i), nn.Sequential(nn.Linear(total_obs + total_action, int(128*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(128*weight_multiplier), int(64*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(64*weight_multiplier), 1)))\n",
    "\n",
    "    def forward(self, obs, action): # Returns the q value evaluations for the agent possible actions\n",
    "        q_values = [torch.empty(obs.shape[0], )] * self.num_agents\n",
    "        x = torch.cat((obs.view(obs.shape[0], obs.shape[1] * obs.shape[2]),\n",
    "                       action.view(action.shape[0], action.shape[1] * action.shape[2])), dim=1)\n",
    "        for agent_i in range(self.num_agents):\n",
    "            q_values[agent_i] = getattr(self, 'agent_{}'.format(agent_i))(x)\n",
    "\n",
    "        return torch.cat(q_values, dim=1)\n",
    "\n",
    "def add_distance_obs(state,n_agents):\n",
    "    sqrt2 = math.sqrt(2)\n",
    "    if n_agents == 2:\n",
    "        dist = math.dist(state[0][1:3],state[1][1:3])/sqrt2 \n",
    "        state[0].append(dist)\n",
    "        state[1].append(dist)\n",
    "        return state\n",
    "    elif n_agents == 3:\n",
    "        distsfor0to1 = math.dist(state[0][1:3],state[1][1:3])/sqrt2 \n",
    "        distsfor0to2 = math.dist(state[0][1:3],state[2][1:3])/sqrt2 \n",
    "        distsfor1to2 = math.dist(state[1][1:3],state[2][1:3])/sqrt2 \n",
    "        state[0].append(distsfor0to1) #1\n",
    "        state[0].append(distsfor0to2) #2\n",
    "        state[1].append(distsfor0to1) #0\n",
    "        state[1].append(distsfor1to2) #2\n",
    "        state[2].append(distsfor0to2) #0\n",
    "        state[2].append(distsfor1to2) #1\n",
    "        return state\n",
    "    elif n_agents == 4:\n",
    "        distsfor0to1 = math.dist(state[0][1:3],state[1][1:3])/sqrt2 \n",
    "        distsfor0to2 = math.dist(state[0][1:3],state[2][1:3])/sqrt2 \n",
    "        distsfor1to2 = math.dist(state[1][1:3],state[2][1:3])/sqrt2 \n",
    "        distsfor0to3 = math.dist(state[0][1:3],state[3][1:3])/sqrt2 \n",
    "        distsfor1to3 = math.dist(state[1][1:3],state[3][1:3])/sqrt2 \n",
    "        distsfor2to3 = math.dist(state[2][1:3],state[3][1:3])/sqrt2 \n",
    "        state[0].append(distsfor0to1) #1\n",
    "        state[0].append(distsfor0to2) #2\n",
    "        state[0].append(distsfor0to3) #3\n",
    "        state[1].append(distsfor0to1) #0\n",
    "        state[1].append(distsfor1to2) #2\n",
    "        state[1].append(distsfor1to3) #3\n",
    "        state[2].append(distsfor0to2) #0\n",
    "        state[2].append(distsfor1to2) #1\n",
    "        state[2].append(distsfor2to3) #3\n",
    "        state[3].append(distsfor0to3) #0\n",
    "        state[3].append(distsfor1to3) #1\n",
    "        state[3].append(distsfor2to3) #2\n",
    "        return state\n",
    "    return \"Only supports 2,3,4 agents\"\n",
    "\n",
    "def test(env, num_episodes, mu, n_agents,render=True): # Does not use the critic network, just uses the actor networks\n",
    "    score = np.zeros(env.n_agents)\n",
    "    epsilon_test = 0.01\n",
    "    with torch.no_grad():\n",
    "        for episode_i in range(num_episodes):\n",
    "            state = add_distance_obs(env.reset(),n_agents)\n",
    "            done = [False for _ in range(env.n_agents)]\n",
    "\n",
    "            while not all(done):\n",
    "                if render == True:\n",
    "                        env.render()\n",
    "                        time.sleep(0.0005)\n",
    "                \n",
    "                if np.random.rand() < epsilon_test:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action_logits = mu(torch.Tensor(state).unsqueeze(0))\n",
    "                    action = action_logits.argmax(dim=2).squeeze(0).data.cpu().numpy().tolist()\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = add_distance_obs(next_state,n_agents) # Added\n",
    "                score += np.array(reward)\n",
    "                state = next_state\n",
    "    return sum(score / num_episodes)\n",
    "\n",
    "def main(env_name, lr_mu, lr_q, tau, gamma, batch_size, buffer_limit, max_episodes, log_interval, test_episodes,\n",
    "         warm_up_steps, update_iter, gumbel_max_temp, gumbel_min_temp, grid_size, n_agents, n_trees, max_steps, agent_view,n_obstacles,step_cost\n",
    "         ,max_steps_without_reward,tree_strength,weight_multiplier,render_interval,tree_cutdown_reward):\n",
    "\n",
    "    load_saved_models = False\n",
    "    \n",
    "    gym.envs.register(\n",
    "        id='my_Lumberjacks-v1',\n",
    "        entry_point='ma_gym.envs.lumberjacks:Lumberjacks', # Points to the lumberjack class object\n",
    "        kwargs={'tree_cutdown_reward':tree_cutdown_reward,'tree_strength':tree_strength, 'max_steps_without_reward':max_steps_without_reward, 'n_obstacles':n_obstacles,'n_agents': n_agents, 'n_trees':n_trees, 'full_observable': False, 'step_cost': step_cost, 'grid_shape':(grid_size,grid_size),'agent_view':agent_view,'max_steps':max_steps} # Add additional args\n",
    "    )\n",
    "\n",
    "    env = gym.make('my_Lumberjacks-v1')\n",
    "    test_env = gym.make('my_Lumberjacks-v1')\n",
    "    # Parameyers to adjust about the environment\n",
    "\n",
    "    mu_target = MuNet(env.observation_space, env.action_space,weight_multiplier)\n",
    "    # Add in the part here to load in the mu network\n",
    "    mu_target.load_state_dict(torch.load('/Users/mingliu/Documents/R Learning/Final Project Code/emilygrid10/G_mu_target_DIST_OBS_agents4_grid10.pt'))\n",
    "\n",
    "\n",
    "    test_score = test(test_env, test_episodes, mu_target, n_agents,render=True)\n",
    "    print(test_score)\n",
    "if __name__ == '__main__':\n",
    "    # Only edit these parts\n",
    " \n",
    "    n_agent = 4# 2,3,4 # Change this\n",
    "    mapsize = 10 # Pair change to 6, Emily change to 10\n",
    "\n",
    "    # DON\"T EDIT BELOW THIS LINE\n",
    "    #----------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------\n",
    "    # DON\"T EDIT BELOW THIS LINE\n",
    "    if mapsize == 6: \n",
    "        num_trees = 12\n",
    "        max_steps = 150\n",
    "        num_obstacles = 5\n",
    "        max_episodes = 30000\n",
    "        multiplier = 1.25\n",
    "    elif mapsize == 10: \n",
    "        num_trees = 36\n",
    "        max_steps = 250\n",
    "        num_obstacles = 12\n",
    "        max_episodes = 30000\n",
    "        multiplier = 1.25\n",
    "    else:\n",
    "        print(\"DO 4 or 6\")\n",
    "    tree_strength = []\n",
    "    for i in range(1,n_agent+1):\n",
    "        tree_strength.extend([int(i)]*int(num_trees / n_agent)) \n",
    "\n",
    "    print(tree_strength)\n",
    "    print(multiplier)\n",
    "    print(max_steps)\n",
    "    print(max_episodes)\n",
    "    print(num_trees)\n",
    "    print(num_obstacles)\n",
    "    \n",
    "    kwargs = {'env_name': 'ma_gym:Lumberjacks-v0',\n",
    "              'lr_mu': 0.0005,                      # Learning rate for Actors\n",
    "              'lr_q': 0.001,                        # Learning rate for Critic\n",
    "              'batch_size': 32,\n",
    "              'tau': 0.005,\n",
    "              'gamma': 0.99,\n",
    "              'buffer_limit': 50000,\n",
    "              'log_interval': 20,\n",
    "              'render_interval': 500,              # Every this many games, show an example of the game\n",
    "              'max_episodes': max_episodes,                # 10000 default\n",
    "              'test_episodes': 3000,\n",
    "              'warm_up_steps': 2000,\n",
    "              'update_iter': 10,\n",
    "              'gumbel_max_temp': 10,\n",
    "              'gumbel_min_temp': 0.1,\n",
    "              'weight_multiplier':multiplier,\n",
    "              \n",
    "              'grid_size' : mapsize,\n",
    "              'n_agents' : n_agent, #3,4 change this\n",
    "              'n_trees' : num_trees,\n",
    "              'tree_strength' : tree_strength        ,  # #Even spread based on the number of agents\n",
    "              'max_steps' : max_steps,\n",
    "              'agent_view' : (2,2),\n",
    "              'n_obstacles' : num_obstacles,\n",
    "              'step_cost' : -0.1,\n",
    "              'tree_cutdown_reward': 10,\n",
    "              'max_steps_without_reward' : 50000 # Don't really use this\n",
    "    }\n",
    "\n",
    "    main(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from ma_gym.wrappers import Monitor\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class MuNet(nn.Module):\n",
    "    # Actor Network\n",
    "    def __init__(self, observation_space, action_space, weight_multiplier = 1):\n",
    "        super(MuNet, self).__init__()\n",
    "        self.num_agents = len(observation_space)\n",
    "        self.action_space = action_space\n",
    "        for agent_i in range(self.num_agents): # For each agent, instantiate a Individual Actor network\n",
    "            n_obs = observation_space[agent_i].shape[0] \n",
    "            print('N_obs in MuNet',n_obs)\n",
    "            num_action = action_space[agent_i].n\n",
    "            setattr(self, 'agent_{}'.format(agent_i), nn.Sequential(nn.Linear(n_obs, int(128*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(128*weight_multiplier), int(64*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(64*weight_multiplier), num_action)))\n",
    "\n",
    "    def forward(self, obs): # Returns the actions of the agents\n",
    "        action_logits = [torch.empty(1, _.n) for _ in self.action_space]\n",
    "        for agent_i in range(self.num_agents):\n",
    "            x = getattr(self, 'agent_{}'.format(agent_i))(obs[:, agent_i, :]).unsqueeze(1)\n",
    "            action_logits[agent_i] = x\n",
    "\n",
    "        return torch.cat(action_logits, dim=1)\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    # Critic network\n",
    "    def __init__(self, observation_space, action_space,weight_multiplier = 1):\n",
    "        super(QNet, self).__init__()\n",
    "        self.num_agents = len(observation_space)\n",
    "        total_action = sum([_.n for _ in action_space])\n",
    "        total_obs = sum([_.shape[0] for _ in observation_space]) + self.num_agents*(self.num_agents-1)\n",
    "        print(\"Total_obs in QNet\",total_obs)\n",
    "        for agent_i in range(self.num_agents):\n",
    "            setattr(self, 'agent_{}'.format(agent_i), nn.Sequential(nn.Linear(total_obs + total_action, int(128*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(128*weight_multiplier), int(64*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(64*weight_multiplier), 1)))\n",
    "\n",
    "    def forward(self, obs, action): # Returns the q value evaluations for the agent possible actions\n",
    "        q_values = [torch.empty(obs.shape[0], )] * self.num_agents\n",
    "        x = torch.cat((obs.view(obs.shape[0], obs.shape[1] * obs.shape[2]),\n",
    "                       action.view(action.shape[0], action.shape[1] * action.shape[2])), dim=1)\n",
    "        for agent_i in range(self.num_agents):\n",
    "            q_values[agent_i] = getattr(self, 'agent_{}'.format(agent_i))(x)\n",
    "\n",
    "        return torch.cat(q_values, dim=1)\n",
    "\n",
    "def add_distance_obs(state,n_agents):\n",
    "    return state\n",
    "\n",
    "def test(env, num_episodes, mu, n_agents,render=False): # Does not use the critic network, just uses the actor networks\n",
    "    score = np.zeros(env.n_agents)\n",
    "    epsilon_test = 0.01\n",
    "    with torch.no_grad():\n",
    "        for episode_i in range(num_episodes):\n",
    "            state = add_distance_obs(env.reset(),n_agents)\n",
    "            done = [False for _ in range(env.n_agents)]\n",
    "\n",
    "            while not all(done):\n",
    "                if render == True:\n",
    "                        env.render()\n",
    "                        time.sleep(0.0005)\n",
    "                \n",
    "                if np.random.rand() < epsilon_test:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action_logits = mu(torch.Tensor(state).unsqueeze(0))\n",
    "                    action = action_logits.argmax(dim=2).squeeze(0).data.cpu().numpy().tolist()\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = add_distance_obs(next_state,n_agents) # Added\n",
    "                score += np.array(reward)\n",
    "                state = next_state\n",
    "    return sum(score / num_episodes)\n",
    "\n",
    "\n",
    "def main(env_name, lr_mu, lr_q, tau, gamma, batch_size, buffer_limit, max_episodes, log_interval, test_episodes,\n",
    "         warm_up_steps, update_iter, gumbel_max_temp, gumbel_min_temp, grid_size, n_agents, n_trees, max_steps, agent_view,n_obstacles,step_cost\n",
    "         ,max_steps_without_reward,tree_strength,weight_multiplier,render_interval,tree_cutdown_reward):\n",
    "\n",
    "    load_saved_models = False\n",
    "    \n",
    "    gym.envs.register(\n",
    "        id='my_Lumberjacks-v1',\n",
    "        entry_point='ma_gym.envs.lumberjacks:Lumberjacks', # Points to the lumberjack class object\n",
    "        kwargs={'tree_cutdown_reward':tree_cutdown_reward,'tree_strength':tree_strength, 'max_steps_without_reward':max_steps_without_reward, 'n_obstacles':n_obstacles,'n_agents': n_agents, 'n_trees':n_trees, 'full_observable': False, 'step_cost': step_cost, 'grid_shape':(grid_size,grid_size),'agent_view':agent_view,'max_steps':max_steps} # Add additional args\n",
    "    )\n",
    "\n",
    "    env = gym.make('my_Lumberjacks-v1')\n",
    "    test_env = gym.make('my_Lumberjacks-v1')\n",
    "    # Parameyers to adjust about the environment\n",
    "\n",
    "    mu_target = MuNet(env.observation_space, env.action_space,weight_multiplier)\n",
    "    # Add in the part here to load in the mu network\n",
    "    mu_target.load_state_dict(torch.load('/Users/mingliu/Documents/R Learning/Final Project Code/emilygrid10/G_mu_target_NO_OBS_agents4_grid10.pt'))\n",
    "\n",
    "    test_score = test(test_env, test_episodes, mu_target, n_agents,render=True)\n",
    "    print(test_score)\n",
    "if __name__ == '__main__':\n",
    "    # Only edit these parts\n",
    " \n",
    "    n_agent = 4# 2,3,4 # Change this\n",
    "    mapsize = 10 # Pair change to 6, Emily change to 10\n",
    "\n",
    "    # DON\"T EDIT BELOW THIS LINE\n",
    "    #----------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------\n",
    "    # DON\"T EDIT BELOW THIS LINE\n",
    "    if mapsize == 6: \n",
    "        num_trees = 12\n",
    "        max_steps = 150\n",
    "        num_obstacles = 5\n",
    "        max_episodes = 30000\n",
    "        multiplier = 1.25\n",
    "    elif mapsize == 10: \n",
    "        num_trees = 36\n",
    "        max_steps = 250\n",
    "        num_obstacles = 12\n",
    "        max_episodes = 30000\n",
    "        multiplier = 1.25\n",
    "    else:\n",
    "        print(\"DO 4 or 6\")\n",
    "    tree_strength = []\n",
    "    for i in range(1,n_agent+1):\n",
    "        tree_strength.extend([int(i)]*int(num_trees / n_agent)) \n",
    "\n",
    "    print(tree_strength)\n",
    "    print(multiplier)\n",
    "    print(max_steps)\n",
    "    print(max_episodes)\n",
    "    print(num_trees)\n",
    "    print(num_obstacles)\n",
    "    \n",
    "    kwargs = {'env_name': 'ma_gym:Lumberjacks-v0',\n",
    "              'lr_mu': 0.0005,                      # Learning rate for Actors\n",
    "              'lr_q': 0.001,                        # Learning rate for Critic\n",
    "              'batch_size': 32,\n",
    "              'tau': 0.005,\n",
    "              'gamma': 0.99,\n",
    "              'buffer_limit': 50000,\n",
    "              'log_interval': 20,\n",
    "              'render_interval': 500,              # Every this many games, show an example of the game\n",
    "              'max_episodes': max_episodes,                # 10000 default\n",
    "              'test_episodes': 3000,\n",
    "              'warm_up_steps': 2000,\n",
    "              'update_iter': 10,\n",
    "              'gumbel_max_temp': 10,\n",
    "              'gumbel_min_temp': 0.1,\n",
    "              'weight_multiplier':multiplier,\n",
    "              \n",
    "              'grid_size' : mapsize,\n",
    "              'n_agents' : n_agent, #3,4 change this\n",
    "              'n_trees' : num_trees,\n",
    "              'tree_strength' : tree_strength        ,  # #Even spread based on the number of agents\n",
    "              'max_steps' : max_steps,\n",
    "              'agent_view' : (2,2),\n",
    "              'n_obstacles' : num_obstacles,\n",
    "              'step_cost' : -0.1,\n",
    "              'tree_cutdown_reward': 10,\n",
    "              'max_steps_without_reward' : 50000 # Don't really use this\n",
    "    }\n",
    "\n",
    "    main(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "import gym\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from ma_gym.wrappers import Monitor\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class MuNet(nn.Module):\n",
    "    # Actor Network\n",
    "    def __init__(self, observation_space, action_space, weight_multiplier = 1):\n",
    "        super(MuNet, self).__init__()\n",
    "        self.num_agents = len(observation_space)\n",
    "        self.action_space = action_space\n",
    "        for agent_i in range(self.num_agents): # For each agent, instantiate a Individual Actor network\n",
    "            n_obs = observation_space[agent_i].shape[0] \n",
    "            print('N_obs in MuNet',n_obs)\n",
    "            num_action = action_space[agent_i].n\n",
    "            setattr(self, 'agent_{}'.format(agent_i), nn.Sequential(nn.Linear(n_obs, int(128*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(128*weight_multiplier), int(64*weight_multiplier)),\n",
    "                                                                    nn.ReLU(),\n",
    "                                                                    nn.Linear(int(64*weight_multiplier), num_action)))\n",
    "\n",
    "    def forward(self, obs): # Returns the actions of the agents\n",
    "        action_logits = [torch.empty(1, _.n) for _ in self.action_space]\n",
    "        for agent_i in range(self.num_agents):\n",
    "            x = getattr(self, 'agent_{}'.format(agent_i))(obs[:, agent_i, :]).unsqueeze(1)\n",
    "            action_logits[agent_i] = x\n",
    "\n",
    "        return torch.cat(action_logits, dim=1)\n",
    "\n",
    "def add_distance_obs(state,n_agents):\n",
    "    return state\n",
    "\n",
    "def test(env, num_episodes, mu, n_agents,render=False): # Does not use the critic network, just uses the actor networks\n",
    "    score = np.zeros(env.n_agents)\n",
    "    epsilon_test = 0.01\n",
    "    with torch.no_grad():\n",
    "        for episode_i in range(num_episodes):\n",
    "            state = add_distance_obs(env.reset(),n_agents)\n",
    "            done = [False for _ in range(env.n_agents)]\n",
    "\n",
    "            while not all(done):\n",
    "                if render == True:\n",
    "                        env.render()\n",
    "                        time.sleep(0.0005)\n",
    "                \n",
    "                if np.random.rand() < epsilon_test:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action_logits = mu(torch.Tensor(state).unsqueeze(0))\n",
    "                    action = action_logits.argmax(dim=2).squeeze(0).data.cpu().numpy().tolist()\n",
    "\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                next_state = add_distance_obs(next_state,n_agents) # Added\n",
    "                score += np.array(reward)\n",
    "                state = next_state\n",
    "    return sum(score / num_episodes)\n",
    "\n",
    "\n",
    "def main(env_name, lr_mu, lr_q, tau, gamma, batch_size, buffer_limit, max_episodes, log_interval, test_episodes,\n",
    "         warm_up_steps, update_iter, gumbel_max_temp, gumbel_min_temp, grid_size, n_agents, n_trees, max_steps, agent_view,n_obstacles,step_cost\n",
    "         ,max_steps_without_reward,tree_strength,weight_multiplier,render_interval,tree_cutdown_reward):\n",
    "\n",
    "    load_saved_models = False\n",
    "    \n",
    "    gym.envs.register(\n",
    "        id='my_Lumberjacks-v1',\n",
    "        entry_point='ma_gym.envs.lumberjacks:Lumberjacks', # Points to the lumberjack class object\n",
    "        kwargs={'tree_cutdown_reward':tree_cutdown_reward,'tree_strength':tree_strength, 'max_steps_without_reward':max_steps_without_reward, 'n_obstacles':n_obstacles,'n_agents': n_agents, 'n_trees':n_trees, 'full_observable': True, 'step_cost': step_cost, 'grid_shape':(grid_size,grid_size),'agent_view':agent_view,'max_steps':max_steps} # Add additional args\n",
    "    )\n",
    "\n",
    "    env = gym.make('my_Lumberjacks-v1')\n",
    "    test_env = gym.make('my_Lumberjacks-v1')\n",
    "    # Parameyers to adjust about the environment\n",
    "\n",
    "    mu_target = MuNet(env.observation_space, env.action_space,weight_multiplier)\n",
    "    # Add in the part here to load in the mu network\n",
    "    mu_target.load_state_dict(torch.load('/Users/mingliu/Documents/R Learning/Final Project Code/emilygrid10/G_mu_target_ALL_OBS_agents4_grid10.pt'))\n",
    "\n",
    "\n",
    "    test_score = test(test_env, test_episodes, mu_target, n_agents,render=True)\n",
    "    print(test_score)\n",
    "if __name__ == '__main__':\n",
    "    # Only edit these parts\n",
    " \n",
    "    n_agent = 4# 2,3,4 # Change this\n",
    "    mapsize = 10 # Pair change to 6, Emily change to 10\n",
    "\n",
    "    # DON\"T EDIT BELOW THIS LINE\n",
    "    #----------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------\n",
    "    #----------------------------------------------------------------------\n",
    "    # DON\"T EDIT BELOW THIS LINE\n",
    "    if mapsize == 6: \n",
    "        num_trees = 12\n",
    "        max_steps = 150\n",
    "        num_obstacles = 5\n",
    "        max_episodes = 30000\n",
    "        multiplier = 1.25\n",
    "    elif mapsize == 10: \n",
    "        num_trees = 36\n",
    "        max_steps = 250\n",
    "        num_obstacles = 12\n",
    "        max_episodes = 30000\n",
    "        multiplier = 1.25\n",
    "    else:\n",
    "        print(\"DO 4 or 6\")\n",
    "    tree_strength = []\n",
    "    for i in range(1,n_agent+1):\n",
    "        tree_strength.extend([int(i)]*int(num_trees / n_agent)) \n",
    "\n",
    "    print(tree_strength)\n",
    "    print(multiplier)\n",
    "    print(max_steps)\n",
    "    print(max_episodes)\n",
    "    print(num_trees)\n",
    "    print(num_obstacles)\n",
    "    \n",
    "    kwargs = {'env_name': 'ma_gym:Lumberjacks-v0',\n",
    "              'lr_mu': 0.0005,                      # Learning rate for Actors\n",
    "              'lr_q': 0.001,                        # Learning rate for Critic\n",
    "              'batch_size': 32,\n",
    "              'tau': 0.005,\n",
    "              'gamma': 0.99,\n",
    "              'buffer_limit': 50000,\n",
    "              'log_interval': 20,\n",
    "              'render_interval': 500,              # Every this many games, show an example of the game\n",
    "              'max_episodes': max_episodes,                # 10000 default\n",
    "              'test_episodes': 3000,\n",
    "              'warm_up_steps': 2000,\n",
    "              'update_iter': 10,\n",
    "              'gumbel_max_temp': 10,\n",
    "              'gumbel_min_temp': 0.1,\n",
    "              'weight_multiplier':multiplier,\n",
    "              \n",
    "              'grid_size' : mapsize,\n",
    "              'n_agents' : n_agent, #3,4 change this\n",
    "              'n_trees' : num_trees,\n",
    "              'tree_strength' : tree_strength        ,  # #Even spread based on the number of agents\n",
    "              'max_steps' : max_steps,\n",
    "              'agent_view' : (2,2),\n",
    "              'n_obstacles' : num_obstacles,\n",
    "              'step_cost' : -0.1,\n",
    "              'tree_cutdown_reward': 10,\n",
    "              'max_steps_without_reward' : 50000 # Don't really use this\n",
    "    }\n",
    "\n",
    "    main(**kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
